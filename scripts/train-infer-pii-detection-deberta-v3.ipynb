{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00918461",
   "metadata": {
    "papermill": {
     "duration": 0.010189,
     "end_time": "2024-03-06T21:24:21.294105",
     "exception": false,
     "start_time": "2024-03-06T21:24:21.283916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🕵🏻‍♀️ PII Detection using LLM (Training + Inference)\n",
    "\n",
    "This notebook fine-tunes LLMs to detect personal identification information (PII) from students essays. The categories include six types: name, email, userid, username, phone number, street address and personal url.\n",
    "\n",
    "In order to increase the diversity of texts, the below datasets are used \n",
    "- @SILVESTRE BAHI [Pii-Mistral-2k-fit-competition](https://www.kaggle.com/datasets/mandrilator/pii-mistral-2k-fit-competition)\n",
    "- @Min-Hsien Weng [AI-generated-text-dataset](https://www.kaggle.com/datasets/minhsienweng/ai-generated-text-dataset). The notebook is [Create AI-generated essays using LLM](https://www.kaggle.com/code/minhsienweng/create-ai-generated-essays-using-llm)\n",
    "- @VALENTIN WERNER [fix punctuation tokenization external dataset](https://www.kaggle.com/code/valentinwerner/fix-punctuation-tokenization-external-dataset))\n",
    "\n",
    "**Model weight**: My trained model weights are stored in the [PII Data Detection Models](https://www.kaggle.com/datasets/minhsienweng/pii-data-detection-models) dataset. If you find it helpful, please upvote it. Thank you! 😁\n",
    "\n",
    "**Change Logs**\n",
    "- 🤩**[Version 83] (LB=0.95)** Performs inference on the entire input without truncation + email and phone detector using regular expression\n",
    "\n",
    "- [Version 58] (LB=0.869) Train the model with 10,889 texts: 6,807 Kaggle texts + 1,390 Gemma texts + 2692 Mixtral8x7B texts and offline install `seqeval` and `evaluation` packages. An email detector is included.\n",
    "- [Version 55] (LB=0.845) Update the data split and Train the model with 8,197 texts: 6,807 Kaggle texts + 1,390 Gemma texts\n",
    "- [Version 48] (LB=0.857) Train the model with 10,197 texts: 6807 Kaggle texts + 2000 Mistral-7b + 1390 Gemma texts\n",
    "- [Version 44] (LB=0.83) Train the model with 6807 Kaggle texts + 2000 texts by @SILVESTRE BAHI [Pii-Mistral-2k-fit-competition](https://www.kaggle.com/datasets/mandrilator/pii-mistral-2k-fit-competition)\n",
    "- [Version 37] (LB=0.855) Train the model with three epochs (the best is the 2nd) \n",
    "- [Version 26] Split the data into training and valid dataset (using `NAME_STUDENT' 'EMAIL' 'STREET_ADDRESS')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699443ae",
   "metadata": {
    "papermill": {
     "duration": 0.0092,
     "end_time": "2024-03-06T21:24:21.313144",
     "exception": false,
     "start_time": "2024-03-06T21:24:21.303944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Offline installation\n",
    "Install `seqeaval` and `evaluation` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae4fa6c",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-03-06T21:24:21.333785Z",
     "iopub.status.busy": "2024-03-06T21:24:21.333416Z",
     "iopub.status.idle": "2024-03-06T21:25:01.490447Z",
     "shell.execute_reply": "2024-03-06T21:25:01.489300Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 40.170327,
     "end_time": "2024-03-06T21:25:01.493000",
     "exception": false,
     "start_time": "2024-03-06T21:24:21.322673",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/seqeval\r\n",
      "Processing /kaggle/input/seqeval/seqeval-1.2.2.tar.gz (from -r /kaggle/input/seqeval/requirements.txt (line 1))\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval->-r /kaggle/input/seqeval/requirements.txt (line 1)) (1.24.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->-r /kaggle/input/seqeval/requirements.txt (line 1)) (1.2.2)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->-r /kaggle/input/seqeval/requirements.txt (line 1)) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->-r /kaggle/input/seqeval/requirements.txt (line 1)) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->-r /kaggle/input/seqeval/requirements.txt (line 1)) (3.2.0)\r\n",
      "Building wheels for collected packages: seqeval\r\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=cb90e1d91ccdf27d939081f9986d614081ffdff9d6d14d9ab452366b657dc0ce\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/be/6d/82/87acaf836bed90667f77936325c0a4b631944650898dee7802\r\n",
      "Successfully built seqeval\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Looking in links: /kaggle/input/evaluate\r\n",
      "Processing /kaggle/input/evaluate/evaluate-0.4.1-py3-none-any.whl (from -r /kaggle/input/evaluate/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (1.24.3)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2.0.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2023.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (0.20.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (0.18.0)\r\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (11.0.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (3.8.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (3.12.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (4.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2023.11.17)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (2023.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (23.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (4.0.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (1.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate->-r /kaggle/input/evaluate/requirements.txt (line 1)) (1.16.0)\r\n",
      "Installing collected packages: evaluate\r\n",
      "Successfully installed evaluate-0.4.1\r\n",
      "Looking in links: /kaggle/input/peft-installation\r\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.4.1)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.24.3)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.0.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.20.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.18.0)\r\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (11.0.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.8.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.12.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.11.17)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (23.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.0.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install --no-index --find-links=/kaggle/input/seqeval -r /kaggle/input/seqeval/requirements.txt\n",
    "# !pip install --no-index --find-links=/kaggle/input/evaluate -r /kaggle/input/evaluate/requirements.txt\n",
    "# !pip install --no-index --find-links=/kaggle/input/peft-installation -r /kaggle/input/peft-installation/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4090f25",
   "metadata": {
    "papermill": {
     "duration": 0.012591,
     "end_time": "2024-03-06T21:25:01.518082",
     "exception": false,
     "start_time": "2024-03-06T21:25:01.505491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba80bf37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:01.543328Z",
     "iopub.status.busy": "2024-03-06T21:25:01.542970Z",
     "iopub.status.idle": "2024-03-06T21:25:19.346250Z",
     "shell.execute_reply": "2024-03-06T21:25:19.345481Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 17.818788,
     "end_time": "2024-03-06T21:25:19.348643",
     "exception": false,
     "start_time": "2024-03-06T21:25:01.529855",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 10:50:36.189382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-17 10:50:36.189468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-17 10:50:36.362269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json, argparse, torch, sys, random, gc, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformer \n",
    "from transformers import (AutoTokenizer, Trainer, TrainingArguments,\n",
    "                          AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "                          DebertaV2Config, DebertaV2ForTokenClassification)\n",
    "from datasets import Dataset, features\n",
    "from typing import Iterable, Any, Callable\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bf5006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:19.374643Z",
     "iopub.status.busy": "2024-03-06T21:25:19.374046Z",
     "iopub.status.idle": "2024-03-06T21:25:19.382658Z",
     "shell.execute_reply": "2024-03-06T21:25:19.381812Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.023286,
     "end_time": "2024-03-06T21:25:19.384526",
     "exception": false,
     "start_time": "2024-03-06T21:25:19.361240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d129f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:19.409509Z",
     "iopub.status.busy": "2024-03-06T21:25:19.409191Z",
     "iopub.status.idle": "2024-03-06T21:25:19.413971Z",
     "shell.execute_reply": "2024-03-06T21:25:19.413141Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.01942,
     "end_time": "2024-03-06T21:25:19.415815",
     "exception": false,
     "start_time": "2024-03-06T21:25:19.396395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc097ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:19.440968Z",
     "iopub.status.busy": "2024-03-06T21:25:19.440655Z",
     "iopub.status.idle": "2024-03-06T21:25:19.497002Z",
     "shell.execute_reply": "2024-03-06T21:25:19.495939Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.071262,
     "end_time": "2024-03-06T21:25:19.499078",
     "exception": false,
     "start_time": "2024-03-06T21:25:19.427816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7663f88",
   "metadata": {
    "papermill": {
     "duration": 0.011981,
     "end_time": "2024-03-06T21:25:19.523290",
     "exception": false,
     "start_time": "2024-03-06T21:25:19.511309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load trainning data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a89281",
   "metadata": {
    "papermill": {
     "duration": 0.011715,
     "end_time": "2024-03-06T21:25:19.546799",
     "exception": false,
     "start_time": "2024-03-06T21:25:19.535084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PII has 13 labels: `'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL' 'O'`\n",
    "\n",
    "1. Loading and Combining Data: load the Kaggle training dataset, moredata, and pii_dataset into a single, unified dataset. Our dataset comprises a total of 16,631 texts: 6,807 from the Kaggle competition, 2,000 about design thinking tools generated by Mistral-7b, and 6,434 about a person generated by Mistral-7b.\n",
    "\n",
    "2. Data Splitting: This dataset is then split into training and validation sets using three columns (''NAME_STUDENT', 'EMAIL', 'STREET_ADDRESS'). 300 of the data is allocated to the training set. The remaining is used for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b59aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:19.572459Z",
     "iopub.status.busy": "2024-03-06T21:25:19.572114Z",
     "iopub.status.idle": "2024-03-06T21:25:19.586249Z",
     "shell.execute_reply": "2024-03-06T21:25:19.585412Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.029362,
     "end_time": "2024-03-06T21:25:19.588150",
     "exception": false,
     "start_time": "2024-03-06T21:25:19.558788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1-hot encoding \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def load_data():\n",
    "    # Load training data\n",
    "    train_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\")\n",
    "    print(f\"kaggle train data = {len(train_data)}\") # 6807\n",
    "    more_data = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\")\n",
    "    print(f\"more data = {len(more_data)}\")\n",
    "    pii_dataset_fixed = pd.read_json(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\")\n",
    "    print(f\"pii_dataset_fixed = {len(pii_dataset_fixed)}\")\n",
    "#     # Texts generated by Gemma\n",
    "#     gemma_df = pd.read_json(\"/kaggle/input/ai-generated-text-dataset/pii_dataset_gemma_1700.json\")\n",
    "#     print(\"gemma data = \", len(gemma_df)) # 1390\n",
    "#     # PII - Mixtral8x7B generated essays (2692)\n",
    "#     df_mpware = pd.read_json('/kaggle/input/ai-generated-text-dataset/pii_dataset_Gemini_NEWTON_BABA.json')  \n",
    "#     df_mpware = df_mpware[train_data.columns]\n",
    "#     print(f\"df_mpware data = {len(df_mpware)}\")\n",
    "    # Combine to a single df\n",
    "    df = pd.concat([train_data, more_data, pii_dataset_fixed])\n",
    "    # df = train_data\n",
    "    df['document'] = [i for i in range(len(df))] # Update the document id\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "     # Get all the unique labels \n",
    "    all_labels = sorted(np.unique(functools.reduce(lambda a, b: list(np.unique(a+b)),\n",
    "                                                  df['labels'].tolist())))\n",
    "    print(f\"all_labels = {all_labels}\")\n",
    "    # Create indexes for labels\n",
    "    label2id = {label:index for index,label in enumerate(all_labels)}\n",
    "    id2label = {index:label for index,label in enumerate(all_labels)}\n",
    "    return df, all_labels, label2id, id2label\n",
    "    \n",
    "# Eencode labels to columns\n",
    "def encode_labels(df):\n",
    "    total = len(df)\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda labels: \n",
    "                                            list(set([label.split('-')[1] for label in labels if label != 'O'])))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    # add 'POS' column that don't have \n",
    "    df['others'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n",
    "    label_classes = list(mlb.classes_) + ['others']\n",
    "    for col in label_classes:\n",
    "        subtotal = df[col].sum()\n",
    "        percent = subtotal/total * 100\n",
    "        print(f'{col}: {subtotal}  ({percent:.1f}%)')\n",
    "    return df, label_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ae5654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:19.612905Z",
     "iopub.status.busy": "2024-03-06T21:25:19.612620Z",
     "iopub.status.idle": "2024-03-06T21:25:37.072104Z",
     "shell.execute_reply": "2024-03-06T21:25:37.071130Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 17.474259,
     "end_time": "2024-03-06T21:25:37.074166",
     "exception": false,
     "start_time": "2024-03-06T21:25:19.599907",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /kaggle/input/pii-detection-removal-from-educational-data/train.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df, all_labels, label2id, id2label \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_labels, label_classes \u001b[38;5;241m=\u001b[39m encode_labels(df\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m      3\u001b[0m df_labels\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_labels.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/pii-detection-removal-from-educational-data/train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle train data = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 6807\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     more_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /kaggle/input/pii-detection-removal-from-educational-data/train.json does not exist"
     ]
    }
   ],
   "source": [
    "df, all_labels, label2id, id2label = load_data()\n",
    "df_labels, label_classes = encode_labels(df.copy())\n",
    "df_labels.to_csv(\"df_labels.csv\", encoding=\"utf-8\")\n",
    "display(df_labels.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b4073",
   "metadata": {
    "papermill": {
     "duration": 0.012897,
     "end_time": "2024-03-06T21:25:37.100450",
     "exception": false,
     "start_time": "2024-03-06T21:25:37.087553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split the data into training and valid data \n",
    "The data split is based on whether labels are all 'O' or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da1aede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:37.129695Z",
     "iopub.status.busy": "2024-03-06T21:25:37.129164Z",
     "iopub.status.idle": "2024-03-06T21:25:37.140097Z",
     "shell.execute_reply": "2024-03-06T21:25:37.138939Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.028863,
     "end_time": "2024-03-06T21:25:37.142274",
     "exception": false,
     "start_time": "2024-03-06T21:25:37.113411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_df_by_sampling(df, n_samples):\n",
    "    # Get the sample df\n",
    "    samples_df = df.sample(n=n_samples, random_state=SEED)\n",
    "    # The remaining df\n",
    "    cond = df['document'].isin(samples_df['document'])\n",
    "    others_df = df.drop(df[cond].index, inplace=False)\n",
    "    return samples_df, others_df\n",
    "\n",
    "def downsample_df(df):\n",
    "    '''Split the df into training and valid dataset'''\n",
    "    df['is_labels'] = df['labels'].apply(lambda labels: any(label != 'O' for label in labels))\n",
    "    # One or more labels are not 'O'\n",
    "    true_labels = df[df['is_labels'] == True]\n",
    "    # all labels are 'O'\n",
    "    false_labels = df[df['is_labels'] == False] \n",
    "    # Reset index to two df\n",
    "    true_labels = true_labels.reset_index(drop=True, inplace=False)\n",
    "    false_labels = false_labels.reset_index(drop=True, inplace=False)\n",
    "    print(f\"Number of true_labels = {len(true_labels)}\")\n",
    "    print(f\"Number of false_labels = {len(false_labels)}\")\n",
    "    # Get 300 as valid dataset\n",
    "    n_samples=len(true_labels) - 150\n",
    "    # Sample true labels\n",
    "    true_samples, true_others = split_df_by_sampling(true_labels, n_samples)\n",
    "    print(f\"true_samples = {len(true_samples)} true_others = {len(true_others)}\")\n",
    "    n_samples=len(false_labels) - 150\n",
    "    # Sample false labels\n",
    "    false_samples, false_others = split_df_by_sampling(false_labels, n_samples)\n",
    "    print(f\"false_samples = {len(false_samples)} false_others = {len(false_others)}\")\n",
    "    # Training ds = P * true_labels + P * false_labels\n",
    "    train_df = pd.concat([true_samples, false_samples])   \n",
    "    # Valid ds = (1-P) * true_labels + (1-P) * false_labels\n",
    "    valid_df = pd.concat([true_others, false_others])   \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4a2122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:37.213148Z",
     "iopub.status.busy": "2024-03-06T21:25:37.212471Z",
     "iopub.status.idle": "2024-03-06T21:25:37.976789Z",
     "shell.execute_reply": "2024-03-06T21:25:37.975688Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.780896,
     "end_time": "2024-03-06T21:25:37.979126",
     "exception": false,
     "start_time": "2024-03-06T21:25:37.198230",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true_labels = 7369\n",
      "Number of false_labels = 5872\n",
      "true_samples = 7219 true_others = 150\n",
      "false_samples = 5722 false_others = 150\n",
      "Number of train_df = 12941\n",
      "Number of valid_df = 300\n"
     ]
    }
   ],
   "source": [
    "# Split 'df' into training and valid dataset (300) based on whether the row is all 'O' or not. \n",
    "train_df, valid_df = downsample_df(df.copy())\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "print(f\"Number of train_df = {len(train_df)}\")\n",
    "print(f\"Number of valid_df = {len(valid_df)}\")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd53d5",
   "metadata": {
    "papermill": {
     "duration": 0.013745,
     "end_time": "2024-03-06T21:25:38.006856",
     "exception": false,
     "start_time": "2024-03-06T21:25:37.993111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization\n",
    "Create the dataset from `train_data` and tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2e985e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.035495Z",
     "iopub.status.busy": "2024-03-06T21:25:38.034865Z",
     "iopub.status.idle": "2024-03-06T21:25:38.044492Z",
     "shell.execute_reply": "2024-03-06T21:25:38.043580Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026256,
     "end_time": "2024-03-06T21:25:38.046597",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.020341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(example, tokenizer, label2id):\n",
    "    # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for token, label, t_ws in zip(example[\"tokens\"], \n",
    "                                  example[\"provided_labels\"],\n",
    "                                  example[\"trailing_whitespace\"]):\n",
    "        tokens.append(token)\n",
    "        labels.extend([label] * len(token))\n",
    "        # Added trailing whitespace and label if true and \n",
    "        if t_ws:\n",
    "            tokens.append(\" \")\n",
    "            labels.append(\"O\")  \n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    # print(f\"len(text)={len(text)}, len(tokens)={len(tokens)}\")\n",
    "    # tokenization without truncation\n",
    "    tokenized = tokenizer(text, return_offsets_mapping=True,\n",
    "                          truncation=False)\n",
    "    labels = np.array(labels)\n",
    "    # Labels\n",
    "    token_labels = []\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # Added 'O' \n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"]) \n",
    "        else:\n",
    "            # case when the text starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "            # Convert label to id (int)\n",
    "            label_id = label2id[labels[start_idx]]\n",
    "            token_labels.append(label_id)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": len(tokenized.input_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f8170",
   "metadata": {
    "papermill": {
     "duration": 0.013108,
     "end_time": "2024-03-06T21:25:38.072981",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.059873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training without Truncation\n",
    "The model's performance is evaluated using F1 scores calculated by the `seqeval` package. This package is specifically designed to measure the accuracy of entity recognition tasks, as can be seen in its documentation: https://pypi.org/project/seqeval/0.0.10/. \n",
    "\n",
    "- The scikit-learn package is not used for this task as it's not well-suited for multi-class classification problems like ours.\n",
    "- PEFT (https://github.com/huggingface/peft) is employed to efficiently train the model by fine-tuning only a small portion of its parameters, reducing costs.\n",
    "\n",
    "**Reference**\n",
    "- @ILYA TURAEV [[.957] Deberta3base Training](https://www.kaggle.com/code/ilya2raev/957-deberta3base-training)\n",
    "\n",
    "\n",
    "- @NISCHAY DHANKHAR [PII LongFormer Train - Pytorch Lightning](https://www.kaggle.com/code/nischaydnk/pii-longformer-train-pytorch-lightning)\n",
    "- @SILVESTRE BAHI [Pii-Mistral-2k-fit-competition](https://www.kaggle.com/datasets/mandrilator/pii-mistral-2k-fit-competition/data)\n",
    "- @VALENTIN WERNER [[.91lr_scheduler_type](https://www.kaggle.com/code/valentinwerner/915-deberta3base-training)\n",
    "- @VALENTIN WERNER [[.915] Deberta3Base Inference](https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788)\n",
    "- @Nicholas Broad [transformer ner baseline [lb 0.854]](https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854)\n",
    "- @Joseph Josia [PIIDD - Deberta Model Starter [Training]](https://www.kaggle.com/code/takanashihumbert/piidd-deberta-model-starter-training)\n",
    "- @OLGA TSYMBOI [[.912] Blending 0.903 + 0.854 Deberta3Base](https://www.kaggle.com/code/olyatsimboy/912-blending-0-903-0-854-deberta3base)\n",
    "- @DARIEN SCHETTLER [TLAL 🔒 PII Data Detection 🕵️ EDA & Learn With Me](https://www.kaggle.com/code/dschettler8845/tlal-pii-data-detection-eda-learn-with-me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645c076",
   "metadata": {
    "papermill": {
     "duration": 0.012667,
     "end_time": "2024-03-06T21:25:38.099064",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.086397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compute metric\n",
    "The modified 'f1' score is used to evaluate the performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c3c697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.126130Z",
     "iopub.status.busy": "2024-03-06T21:25:38.125818Z",
     "iopub.status.idle": "2024-03-06T21:25:38.140287Z",
     "shell.execute_reply": "2024-03-06T21:25:38.139556Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.030323,
     "end_time": "2024-03-06T21:25:38.142239",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.111916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import recall_score, precision_score, f1_score\n",
    "# Compute the model performance metrics using `seqeval`\n",
    "def compute_metrics(preds, all_labels):    \n",
    "    try:\n",
    "        #print(\"Compute metrics\")\n",
    "        predictions, labels = preds\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        # Include prediction Remove ignored index (special tokens)\n",
    "        true_preds = []\n",
    "        true_labels = []\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_preds.append([all_labels[p] for p, l in zip(pred, label) if l != -100])\n",
    "            true_labels.append([all_labels[l] for p, l in zip(pred, label) if l != -100])\n",
    "        # Compute recall, precision and f1 score\n",
    "        recall = recall_score(true_labels, true_predictions)\n",
    "        precision = precision_score(true_labels, true_predictions)\n",
    "        # Use modified f1 score to measure the performance\n",
    "        f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        result = {'f1': f1_score,  \n",
    "                  'recall': recall,\n",
    "                  'precision': precision}\n",
    "        print(f\"result = {result}\")\n",
    "        return result\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab0b5bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.170613Z",
     "iopub.status.busy": "2024-03-06T21:25:38.169929Z",
     "iopub.status.idle": "2024-03-06T21:25:38.185784Z",
     "shell.execute_reply": "2024-03-06T21:25:38.184920Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.032065,
     "end_time": "2024-03-06T21:25:38.187710",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.155645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.num_proc = 3\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_train_epochs = 3 # Number of epochs\n",
    "        self.batch_size = 1 # Too large batch sizes lead to OOM\n",
    "        self.fp16 = True if torch.cuda.is_available() else False\n",
    "        self.model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\"\n",
    "        self.output_dir = \"/kaggle/working/output\"\n",
    "        self.save_path = f\"/kaggle/working/deberta3base_truncation_false\"\n",
    "        self.load_model()\n",
    "        \n",
    "    # Load the model\n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path) \n",
    "        # Load tokenizer config\n",
    "        config = DebertaV2Config.from_pretrained(self.model_path)       \n",
    "        # Increase context length using the max_position_embeddings parameter \n",
    "        config.update({\n",
    "            'num_labels': len(self.all_labels),\n",
    "            'id2label': self.id2label,\n",
    "            'label2id': self.label2id,\n",
    "            'ignore_mismatched_sizes': True,\n",
    "        })\n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path,\n",
    "                                                                     config=config)\n",
    "        print(f\"Complete loading pretrained LLM model\") \n",
    "        \n",
    "    # Convert df to tokenized dataset\n",
    "    def create_dataset(self, df):\n",
    "        ds = Dataset.from_dict({\n",
    "            \"full_text\": df[\"full_text\"].tolist() ,\n",
    "            \"document\": df[\"document\"].astype('string'),\n",
    "            \"tokens\": df[\"tokens\"].tolist(),\n",
    "            \"trailing_whitespace\": df[\"trailing_whitespace\"].tolist(),\n",
    "            \"provided_labels\": df[\"labels\"].tolist()\n",
    "        })\n",
    "         # Tokenize the dataset\n",
    "        tokenized_ds = ds.map(tokenize, \n",
    "                              fn_kwargs={\"tokenizer\": self.tokenizer, \n",
    "                                         \"label2id\": self.label2id},\n",
    "                              num_proc=self.num_proc)\n",
    "        return tokenized_ds\n",
    "    \n",
    "    # Train the model\n",
    "    def train(self, train_df, valid_df):       \n",
    "        # Create training dataset\n",
    "        training_ds = self.create_dataset(train_df)\n",
    "        # Create valid dataset\n",
    "        valid_ds = self.create_dataset(valid_df)\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer, pad_to_multiple_of=16)               \n",
    "        # Trainer cofiguration\n",
    "        training_args = TrainingArguments(output_dir=self.output_dir, \n",
    "                                          fp16=self.fp16, # # Change to False if using CPU only\n",
    "                                          learning_rate=self.learning_rate,\n",
    "                                          num_train_epochs=self.num_train_epochs, # The total number of training epochs to run.\n",
    "                                          per_device_train_batch_size=self.batch_size,  # batch size per device during training\n",
    "                                          per_device_eval_batch_size=self.batch_size, # batch size for evaluation\n",
    "                                          gradient_accumulation_steps=2, \n",
    "                                          report_to=\"none\",\n",
    "                                          evaluation_strategy=\"epoch\", # Evaluated at the end of epochs\n",
    "                                          # eval_steps=1,\n",
    "                                          do_eval=True,\n",
    "                                          save_strategy=\"epoch\",\n",
    "                                          save_total_limit=2, # Save the best and most recent checkpoints\n",
    "                                          logging_steps=20,\n",
    "                                          lr_scheduler_type='cosine',\n",
    "                                          load_best_model_at_end=True, # Load the best model at the end\n",
    "                                          metric_for_best_model=\"f1\",\n",
    "                                          greater_is_better=True,\n",
    "                                          warmup_ratio=0.1, # number of warmup steps (0.1) for learning rate scheduler\n",
    "                                          weight_decay=0.01, # strength of weight decay\n",
    "                                         )\n",
    "        # Pass the modelTrainer\n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=training_args, \n",
    "                          train_dataset=training_ds,\n",
    "                          eval_dataset=valid_ds, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer,\n",
    "                          compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    "                         )\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        # Save the model\n",
    "        trainer.save_model(self.save_path)\n",
    "        self.tokenizer.save_pretrained(self.save_path)\n",
    "        print(f\"Save the model to {self.save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a96524",
   "metadata": {
    "papermill": {
     "duration": 0.012866,
     "end_time": "2024-03-06T21:25:38.213413",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.200547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b49d23b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.240025Z",
     "iopub.status.busy": "2024-03-06T21:25:38.239728Z",
     "iopub.status.idle": "2024-03-06T21:25:38.244314Z",
     "shell.execute_reply": "2024-03-06T21:25:38.243472Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.020126,
     "end_time": "2024-03-06T21:25:38.246180",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.226054",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training requires the GPUs and internet\n",
    "TRAINING = False # True: Model Training, False: Model Inference\n",
    "if TRAINING: \n",
    "    # Configuration class containing various model and training parameters\n",
    "    trainer = ModelTrainer(all_labels, label2id, id2label)\n",
    "    trainer.train(train_df, valid_df)\n",
    "    sys.exit(0) # Terminate the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d2192",
   "metadata": {
    "papermill": {
     "duration": 0.012614,
     "end_time": "2024-03-06T21:25:38.271628",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.259014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model inference (no truncation) \n",
    "Use the trained model to detect PII information and convert the result to the required submission format.\n",
    "\n",
    "The model inference is inspired by [[.957] DeBERTa-v3-base Infer truncation=False](https://www.kaggle.com/code/ilya2raev/957-deberta-v3-base-infer-truncation-false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1a3ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.298163Z",
     "iopub.status.busy": "2024-03-06T21:25:38.297905Z",
     "iopub.status.idle": "2024-03-06T21:25:38.301922Z",
     "shell.execute_reply": "2024-03-06T21:25:38.301122Z"
    },
    "papermill": {
     "duration": 0.019385,
     "end_time": "2024-03-06T21:25:38.303725",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.284340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STRIDE=384\n",
    "MAX_LENGTH = 1024\n",
    "model_path = \"/kaggle/input/pii-deberta3-base-no-truncation/deberta3base_1024\"\n",
    "threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "021fd34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.331069Z",
     "iopub.status.busy": "2024-03-06T21:25:38.330354Z",
     "iopub.status.idle": "2024-03-06T21:25:38.336506Z",
     "shell.execute_reply": "2024-03-06T21:25:38.335717Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.021877,
     "end_time": "2024-03-06T21:25:38.338414",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.316537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    tokens = []\n",
    "    token_map = []\n",
    "    idx = 0\n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        tokens.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            tokens.append(\" \")\n",
    "            token_map.append(-1) \n",
    "        idx += 1\n",
    "    # Does not truncate the text and concate all words together \n",
    "    # Do not need to have extra space as we have already include it in the previous tokenization\n",
    "    tokenized = tokenizer(\"\".join(tokens), return_offsets_mapping=True, truncation=False)\n",
    "    return {**tokenized, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e14faeb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.366651Z",
     "iopub.status.busy": "2024-03-06T21:25:38.366361Z",
     "iopub.status.idle": "2024-03-06T21:25:38.378249Z",
     "shell.execute_reply": "2024-03-06T21:25:38.377344Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.028053,
     "end_time": "2024-03-06T21:25:38.380168",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.352115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from datasets import Dataset \n",
    "# Model Inferer\n",
    "class ModelInfer:\n",
    "    def __init__(self, all_labels, label2id, id2label):\n",
    "        self.all_labels = all_labels\n",
    "        self.label2id = label2id\n",
    "        self.id2label = id2label\n",
    "        self.model_path = model_path\n",
    "        self.max_length = 1024\n",
    "        self.infer_dir = \"/kaggle/working/infer\" # Model infer output \n",
    "        self.num_proc = 3 # 3 processors\n",
    "        self.threshold = threshold # Threashold\n",
    "        self.load_model()\n",
    "        \n",
    "    def load_model(self):\n",
    "        # Create the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path) \n",
    "        # Create the model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)        \n",
    "#         # Load the fine-tuned adapter layer on top of base model\n",
    "#         self.model = self.model.to(DEVICE)\n",
    "        print(f\"Complete loading pretrained LLM model\")\n",
    "    \n",
    "    def post_processing_preds(self, preds):\n",
    "        preds_final = []\n",
    "        preds_softmax = np.exp(preds) / np.sum(np.exp(preds), axis=2).reshape(preds.shape[0],\n",
    "                                                                              preds.shape[1],\n",
    "                                                                              1)\n",
    "        # Get the maximal value as the final preds\n",
    "        preds = preds.argmax(-1)\n",
    "        preds_without_O = preds_softmax[:,:,:12].argmax(-1) # Prob of entity labels (like 'NAME_STUDENT')\n",
    "        O_preds = preds_softmax[:,:,12] # Prob for 'O'\n",
    "        print()\n",
    "        # If preds for 'O' > 0.99, select preds of 'O'. Otherwise, select preds for entity labels.  \n",
    "        preds_final = np.where(O_preds < self.threshold, preds_without_O, preds)\n",
    "        return preds_final        \n",
    "        \n",
    "    def infer_preds(self, ds):\n",
    "        # Tokenize the dataset using customized Tokenizer (the same as Training Tokenizer)\n",
    "        tokenized_ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": self.tokenizer}, num_proc=2)\n",
    "        # Create data loader\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer,\n",
    "                                                           pad_to_multiple_of=16)\n",
    "        # Arguments (infer only)\n",
    "        args = TrainingArguments(output_dir=self.infer_dir,\n",
    "                                 per_device_eval_batch_size=1, \n",
    "                                 report_to=\"none\")\n",
    "        # Create the trainer \n",
    "        trainer = Trainer(model=self.model, \n",
    "                          args=args, \n",
    "                          data_collator=data_collator, \n",
    "                          tokenizer=self.tokenizer)\n",
    "        \n",
    "        # predict for that split\n",
    "        preds = trainer.predict(tokenized_ds).predictions\n",
    "                \n",
    "        # Clear the unused memory\n",
    "        del self.model, data_collator, trainer, args \n",
    "        clear_memory()\n",
    "        preds_final = self.post_processing_preds(preds)\n",
    "        return preds_final, tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c112adb",
   "metadata": {
    "papermill": {
     "duration": 0.01282,
     "end_time": "2024-03-06T21:25:38.406118",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.393298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load the model and infer the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5914df45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:38.434040Z",
     "iopub.status.busy": "2024-03-06T21:25:38.433298Z",
     "iopub.status.idle": "2024-03-06T21:25:47.609208Z",
     "shell.execute_reply": "2024-03-06T21:25:47.607963Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 9.192021,
     "end_time": "2024-03-06T21:25:47.611246",
     "exception": false,
     "start_time": "2024-03-06T21:25:38.419225",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test dataset 10\n",
      "Complete loading pretrained LLM model\n",
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8039ad6cafcc4651942fb7b0456813d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb17212bc5b45bb9c72faef8235e17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"full_text\": test_data[\"full_text\"].tolist(),\n",
    "    \"document\": test_data[\"document\"].tolist(),\n",
    "    \"tokens\": test_data[\"tokens\"].tolist(),\n",
    "    \"trailing_whitespace\": test_data[\"trailing_whitespace\"].tolist(),\n",
    "})\n",
    "print(f\"Total number of test dataset {len(test_ds)}\")\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "# Load the pretrained model and make the predictions\n",
    "inferer = ModelInfer(all_labels, label2id, id2label)\n",
    "preds_final, tokenized_ds = inferer.infer_preds(test_ds) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c0429",
   "metadata": {
    "papermill": {
     "duration": 0.013912,
     "end_time": "2024-03-06T21:25:47.639735",
     "exception": false,
     "start_time": "2024-03-06T21:25:47.625823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Convert the predicted results to submission df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2127100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:47.669998Z",
     "iopub.status.busy": "2024-03-06T21:25:47.669328Z",
     "iopub.status.idle": "2024-03-06T21:25:47.771600Z",
     "shell.execute_reply": "2024-03-06T21:25:47.770711Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.120498,
     "end_time": "2024-03-06T21:25:47.774332",
     "exception": false,
     "start_time": "2024-03-06T21:25:47.653834",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert preds to a list of dictionaries\n",
    "results = []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final,\n",
    "                                              tokenized_ds[\"token_map\"], \n",
    "                                              tokenized_ds[\"offset_mapping\"],\n",
    "                                              tokenized_ds[\"tokens\"],\n",
    "                                              tokenized_ds[\"document\"]):\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        try:\n",
    "            label_pred = id2label[str(token_pred)]\n",
    "            if start_idx + end_idx == 0: \n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map): \n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                results.append({\n",
    "                        \"document\": doc,\n",
    "                        \"token\": token_id,\n",
    "                        \"label\": label_pred,\n",
    "                        \"token_str\": tokens[token_id]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            print(f\"token_map {len(token_map)} and {token_pred}  {start_idx} {end_idx}\")\n",
    "            sys.exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c4171",
   "metadata": {
    "papermill": {
     "duration": 0.014459,
     "end_time": "2024-03-06T21:25:47.803671",
     "exception": false,
     "start_time": "2024-03-06T21:25:47.789212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Detect emails and phone using regular expressions\n",
    "Inspired by [DeBERTa-v3-Base Striding regex ☎️ & ✉️ Inference](https://www.kaggle.com/code/emiz6413/deberta-v3-base-striding-regex-inference)\n",
    "Improve the accuracy of email and phone numbers by using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e21f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:47.833736Z",
     "iopub.status.busy": "2024-03-06T21:25:47.833369Z",
     "iopub.status.idle": "2024-03-06T21:25:50.826673Z",
     "shell.execute_reply": "2024-03-06T21:25:50.825659Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.01133,
     "end_time": "2024-03-06T21:25:50.829217",
     "exception": false,
     "start_time": "2024-03-06T21:25:47.817887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        \n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "\n",
    "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "emails = []\n",
    "phone_nums = []\n",
    "\n",
    "for _data in test_ds:\n",
    "    # email\n",
    "    for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "        if re.fullmatch(email_regex, token) is not None:\n",
    "            emails.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "            )\n",
    "    # phone number\n",
    "    matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "    if not matches:\n",
    "        continue\n",
    "        \n",
    "    for match in matches:\n",
    "        target = [t.text for t in nlp.tokenizer(match)]\n",
    "        matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        \n",
    "    for matched_span in matched_spans:\n",
    "        for intermediate, token_idx in enumerate(matched_span):\n",
    "            prefix = \"I\" if intermediate else \"B\"\n",
    "            phone_nums.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "            )\n",
    "            \n",
    "results.extend(emails)\n",
    "results.extend(phone_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "777467d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:50.861380Z",
     "iopub.status.busy": "2024-03-06T21:25:50.860693Z",
     "iopub.status.idle": "2024-03-06T21:25:50.867738Z",
     "shell.execute_reply": "2024-03-06T21:25:50.866933Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.025065,
     "end_time": "2024-03-06T21:25:50.869551",
     "exception": false,
     "start_time": "2024-03-06T21:25:50.844486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    # Sort by the document and token\n",
    "    df.sort_values(by=['document', 'token'])\n",
    "    # Combine three columns \n",
    "    df['triplet'] = df[[\"document\", \"token\", \"label\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "    # display(df)\n",
    "    # Drop duplicated triplets and keep the first one as unique row\n",
    "    df = df.drop_duplicates(subset=[\"triplet\"], keep='first')\n",
    "    # Regenerate 'row_id'\n",
    "    df['row_id'] = list(range(len(df)))    \n",
    "    df = df.reset_index(drop=True, inplace=False) \n",
    "    print(\"Remove duplicates\")\n",
    "#     display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30947edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-06T21:25:50.900217Z",
     "iopub.status.busy": "2024-03-06T21:25:50.899891Z",
     "iopub.status.idle": "2024-03-06T21:25:50.926977Z",
     "shell.execute_reply": "2024-03-06T21:25:50.925921Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.045679,
     "end_time": "2024-03-06T21:25:50.929839",
     "exception": false,
     "start_time": "2024-03-06T21:25:50.884160",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26/3957314212.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_id'] = list(range(len(df)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>741</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>742</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>464</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>465</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>104</td>\n",
       "      <td>9</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>112</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>123</td>\n",
       "      <td>32</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>123</td>\n",
       "      <td>33</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>123</td>\n",
       "      <td>1500</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>123</td>\n",
       "      <td>1581</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_id  document  token           label\n",
       "0        0         7      9  B-NAME_STUDENT\n",
       "1        1         7     10  I-NAME_STUDENT\n",
       "2        2         7     52  B-NAME_STUDENT\n",
       "3        3         7    482  B-NAME_STUDENT\n",
       "4        4         7    483  I-NAME_STUDENT\n",
       "5        5         7    741  B-NAME_STUDENT\n",
       "6        6         7    742  I-NAME_STUDENT\n",
       "7        7        10      0  B-NAME_STUDENT\n",
       "8        8        10      1  I-NAME_STUDENT\n",
       "9        9        10    464  B-NAME_STUDENT\n",
       "10      10        10    465  I-NAME_STUDENT\n",
       "11      11        16      4  B-NAME_STUDENT\n",
       "12      12        16      5  I-NAME_STUDENT\n",
       "13      13        20      5  B-NAME_STUDENT\n",
       "14      14        20      6  I-NAME_STUDENT\n",
       "15      15        56     12  B-NAME_STUDENT\n",
       "16      16        56     13  I-NAME_STUDENT\n",
       "17      17        86      6  B-NAME_STUDENT\n",
       "18      18        86      7  I-NAME_STUDENT\n",
       "19      19        93      0  B-NAME_STUDENT\n",
       "20      20        93      1  I-NAME_STUDENT\n",
       "21      21       104      8  B-NAME_STUDENT\n",
       "22      22       104      9  I-NAME_STUDENT\n",
       "23      23       112      5  B-NAME_STUDENT\n",
       "24      24       112      6  I-NAME_STUDENT\n",
       "25      25       123     32  B-NAME_STUDENT\n",
       "26      26       123     33  I-NAME_STUDENT\n",
       "27      27       123   1500  B-NAME_STUDENT\n",
       "28      28       123   1581  B-NAME_STUDENT"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(results)\n",
    "test_df = remove_duplicates(test_df)\n",
    "test_df = test_df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "# Create submission df\n",
    "test_df.to_csv(\"submission.csv\", index=False)\n",
    "display(test_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 2663421,
     "sourceId": 4620664,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4459964,
     "sourceId": 7659420,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4486003,
     "sourceId": 7692250,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4388342,
     "sourceId": 7734200,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4460896,
     "sourceId": 7771858,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4552015,
     "sourceId": 7779062,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 163088908,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 164405654,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 164766955,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 164767167,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 165316169,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 96.566397,
   "end_time": "2024-03-06T21:25:54.453517",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-06T21:24:17.887120",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06cdfc6d7146459383b5e440eb89e0ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7546cd30b51d444ab869788469fdbe6b",
       "placeholder": "​",
       "style": "IPY_MODEL_68f6e3b5cfe4454fa2f4930b447db36e",
       "value": " 5/5 [00:00&lt;00:00, 55.21ex/s]"
      }
     },
     "0db4f8329ccf4706ab4c84f2397e69b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "135f465409b145bd964c2e24ee7ad102": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "17ce52bd476b47b48fb1dbe9ee653994": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2641914f6b75479cb815d07601dc3d08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2cb17212bc5b45bb9c72faef8235e17d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f1ff1adf1c7d4bfda9374c3767f53fa2",
        "IPY_MODEL_ba3b1965a28245bba1ef67313200f153",
        "IPY_MODEL_9881b4edac864dc88d8195f2e5cd1db1"
       ],
       "layout": "IPY_MODEL_2641914f6b75479cb815d07601dc3d08"
      }
     },
     "3dded3c41aa24b6e9a6528feb484af39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4066c8c05a4341b99659a1c02d95dc46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9ba290b4e1b04277bf6b8d9ea68c995e",
       "placeholder": "​",
       "style": "IPY_MODEL_a1ca0a52da4842b2b5b7721d9be5fbda",
       "value": "#0: 100%"
      }
     },
     "68f6e3b5cfe4454fa2f4930b447db36e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7546cd30b51d444ab869788469fdbe6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8039ad6cafcc4651942fb7b0456813d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4066c8c05a4341b99659a1c02d95dc46",
        "IPY_MODEL_da90cab1d8584191a78ec516ee8b6dcd",
        "IPY_MODEL_06cdfc6d7146459383b5e440eb89e0ae"
       ],
       "layout": "IPY_MODEL_17ce52bd476b47b48fb1dbe9ee653994"
      }
     },
     "8cae121e84a942ca88efab752e9f8f7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9881b4edac864dc88d8195f2e5cd1db1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0db4f8329ccf4706ab4c84f2397e69b1",
       "placeholder": "​",
       "style": "IPY_MODEL_8cae121e84a942ca88efab752e9f8f7d",
       "value": " 5/5 [00:00&lt;00:00, 53.10ex/s]"
      }
     },
     "9ba290b4e1b04277bf6b8d9ea68c995e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1ca0a52da4842b2b5b7721d9be5fbda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ba3b1965a28245bba1ef67313200f153": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_eb55a371a1f14b44b3b64e71ceb9b095",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3dded3c41aa24b6e9a6528feb484af39",
       "value": 5
      }
     },
     "da90cab1d8584191a78ec516ee8b6dcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_135f465409b145bd964c2e24ee7ad102",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e180f34199c04582903def8b8f3c044d",
       "value": 5
      }
     },
     "e1452bfc45fe4c1fa8a0dd911515780a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e180f34199c04582903def8b8f3c044d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "eb55a371a1f14b44b3b64e71ceb9b095": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1ff1adf1c7d4bfda9374c3767f53fa2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e1452bfc45fe4c1fa8a0dd911515780a",
       "placeholder": "​",
       "style": "IPY_MODEL_f44a6bd7ce5b4ac2a598c45673e226c1",
       "value": "#1: 100%"
      }
     },
     "f44a6bd7ce5b4ac2a598c45673e226c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
